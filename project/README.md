# InternetWhisper üåêü§ñ

> *Un chatbot de IA avanzado con capacidades de b√∫squeda y recuperaci√≥n de informaci√≥n en tiempo real desde Internet*

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104.0-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.27.2-red.svg)](https://streamlit.io)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìã Tabla de Contenidos

- [Descripci√≥n del Proyecto](#-descripci√≥n-del-proyecto)
- [Arquitectura T√©cnica](#-arquitectura-t√©cnica)
- [Flujo de Datos](#-flujo-de-datos)
- [Dependencias Principales](#-dependencias-principales)
- [Configuraci√≥n del Entorno](#-configuraci√≥n-del-entorno)
- [Instalaci√≥n y Ejecuci√≥n](#-instalaci√≥n-y-ejecuci√≥n)
- [API Documentation](#-api-documentation)
- [Ejemplos de Uso](#-ejemplos-de-uso)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Contribuci√≥n](#-contribuci√≥n)

---

## üéØ Descripci√≥n del Proyecto

**InternetWhisper** es un sistema RAG (Retrieval-Augmented Generation) que combina la b√∫squeda inteligente en Internet con capacidades avanzadas de generaci√≥n de texto mediante IA. A diferencia de los chatbots tradicionales, InternetWhisper no se limita a informaci√≥n preentrenada, sino que busca, procesa y sintetiza informaci√≥n actualizada de la web en tiempo real.

### üöÄ Caracter√≠sticas Principales

- **üîç B√∫squeda Inteligente**: Integraci√≥n con Google Custom Search API
- **üï∑Ô∏è Web Scraping Avanzado**: Extracci√≥n de contenido con Playwright
- **üß† Procesamiento de Lenguaje Natural**: Embeddings y an√°lisis sem√°ntico con OpenAI
- **üíæ Cache Vectorial**: Almacenamiento eficiente en Redis con b√∫squeda por similitud
- **‚ö° Streaming en Tiempo Real**: Respuestas progresivas v√≠a Server-Sent Events
- **üé® Interfaz Interactiva**: UI moderna construida con Streamlit
- **üê≥ Arquitectura Containerizada**: Despliegue sencillo con Docker Compose

---

## üèóÔ∏è Arquitectura T√©cnica

### Patr√≥n de Microservicios

InternetWhisper implementa una arquitectura de microservicios con tres componentes principales:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Frontend     ‚îÇ    ‚îÇ  Orchestrator   ‚îÇ    ‚îÇ     Scraper     ‚îÇ
‚îÇ   (Streamlit)   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (FastAPI)     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (FastAPI)     ‚îÇ
‚îÇ   Puerto: 8501  ‚îÇ    ‚îÇ   Puerto: 8000  ‚îÇ    ‚îÇ   Puerto: 8002  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      Redis      ‚îÇ
                    ‚îÇ  (Vector Cache) ‚îÇ
                    ‚îÇ   Puerto: 6379  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Arquitectura RAG (Retrieval-Augmented Generation)

```
Query ‚Üí [Cache Check] ‚Üí [Web Search] ‚Üí [Scraping] ‚Üí [Text Processing] 
   ‚Üì         ‚Üì              ‚Üì            ‚Üì              ‚Üì
Context ‚Üê [Vector Store] ‚Üê [Embeddings] ‚Üê [Chunking] ‚Üê [Content]
   ‚Üì
[LLM Generation] ‚Üí Response Stream
```

### Componentes T√©cnicos

#### 1. **Frontend Service** ([`src/frontend/main.py`](project/src/frontend/main.py))
- **Framework**: Streamlit
- **Funcionalidad**: Interfaz de usuario interactiva con chat en tiempo real
- **Comunicaci√≥n**: HTTP + SSE con el Orchestrator

#### 2. **Orchestrator Service** ([`src/orchestrator/main.py`](project/src/orchestrator/main.py))
- **Framework**: FastAPI
- **Funcionalidad**: N√∫cleo del sistema RAG, coordina todos los componentes
- **Responsabilidades**:
  - Gesti√≥n del cache vectorial ([`RedisVectorCache`](project/src/orchestrator/retrieval/cache.py))
  - B√∫squeda web ([`GoogleAPI`](project/src/orchestrator/retrieval/search.py))
  - Procesamiento de embeddings ([`OpenAIEmbeddings`](project/src/orchestrator/retrieval/embeddings.py))
  - Generaci√≥n de respuestas con GPT-3.5-turbo

#### 3. **Scraper Service** ([`src/scraper/main.py`](project/src/scraper/main.py))
- **Framework**: FastAPI + Playwright
- **Funcionalidad**: Extracci√≥n de contenido web din√°mico
- **Capacidades**: Manejo de JavaScript, timeouts configurables

---

## üîÑ Flujo de Datos

```mermaid
graph TD
    A[Usuario env√≠a consulta] --> B[Frontend captura input]
    B --> C[Orchestrator recibe query]
    C --> D{Cache vectorial tiene informaci√≥n similar?}
    
    D -->|S√≠, score > 0.85| E[Recupera documentos del cache]
    D -->|No| F[Busca en Google Custom Search]
    
    F --> G[Scraper extrae contenido]
    G --> H[Procesa y divide texto en chunks]
    H --> I[Genera embeddings con OpenAI]
    I --> J[Almacena en Redis Vector Cache]
    J --> E
    
    E --> K[Construye contexto para LLM]
    K --> L[Genera respuesta con GPT-3.5-turbo]
    L --> M[Stream respuesta al usuario]
```

### Detalles del Procesamiento

1. **B√∫squeda y Filtrado**: [`GoogleAPI`](project/src/orchestrator/retrieval/search.py) encuentra URLs relevantes
2. **Extracci√≥n de Contenido**: [`ScraperLocal`](project/src/orchestrator/retrieval/scraper.py)/[`ScraperRemote`](project/src/orchestrator/retrieval/scraper.py) procesan p√°ginas web
3. **Segmentaci√≥n de Texto**: [`LangChainSplitter`](project/src/orchestrator/retrieval/splitter.py) y [`AdjSenSplitter`](project/src/orchestrator/retrieval/splitter.py) dividen contenido
4. **Vectorizaci√≥n**: [`OpenAIEmbeddings`](project/src/orchestrator/retrieval/embeddings.py) genera representaciones sem√°nticas
5. **Almacenamiento**: [`RedisVectorCache`](project/src/orchestrator/retrieval/cache.py) guarda con √≠ndices de similitud
6. **Recuperaci√≥n**: B√∫squeda por similitud coseno (threshold configurable)
7. **Generaci√≥n**: Contexto enriquecido alimenta a GPT-3.5-turbo

---

## üì¶ Dependencias Principales

### Core Dependencies
- **FastAPI**: Framework web async para APIs REST
- **Streamlit**: Framework para interfaces de usuario interactivas
- **OpenAI**: Cliente para GPT-3.5-turbo y text-embedding-ada-002
- **Redis**: Base de datos en memoria con capacidades de b√∫squeda vectorial
- **Playwright**: Automatizaci√≥n de navegadores para web scraping

### AI/ML Stack
- **LangChain**: Framework para aplicaciones LLM y text splitting
- **spaCy**: Procesamiento de lenguaje natural y an√°lisis sem√°ntico
- **scikit-learn**: M√©tricas de similitud coseno
- **pandas**: Manipulaci√≥n de datos estructurados
- **numpy**: Operaciones matem√°ticas con vectores

### Web & Networking
- **aiohttp**: Cliente HTTP as√≠ncrono
- **SSE-Starlette**: Server-Sent Events para streaming
- **BeautifulSoup**: Parsing HTML
- **sseclient-py**: Cliente SSE para frontend

---

## ‚öôÔ∏è Configuraci√≥n del Entorno

### Variables de Entorno Requeridas

Crea un archivo `.env` en el directorio `project/` basado en [`.env.example`](project/.env.example):

```bash
# APIs Configuration
OPENAI_API_KEY=sk-...                                    # Tu API key de OpenAI
GOOGLE_API_KEY=AIza...                                   # Tu API key de Google
GOOGLE_CX=017576...                                      # Tu Custom Search Engine ID

# Google Search Configuration  
GOOGLE_API_HOST="https://www.googleapis.com/customsearch/v1?"
GOOGLE_FIELDS="items(title, displayLink, link, snippet,pagemap/cse_thumbnail)"

# HTTP Headers (para web scraping)
HEADER_ACCEPT_ENCODING="gzip"
HEADER_USER_AGENT="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
```

### Obtenci√≥n de API Keys

#### OpenAI API Key
1. Visita [platform.openai.com](https://platform.openai.com)
2. Crea una cuenta o inicia sesi√≥n
3. Ve a "API Keys" en tu dashboard
4. Genera una nueva clave secreta

#### Google Custom Search Setup
1. Ve a [Google Cloud Console](https://console.cloud.google.com)
2. Crea un nuevo proyecto o selecciona uno existente
3. Habilita la "Custom Search JSON API"
4. Crea credenciales (API Key)
5. Ve a [Programmable Search Engine](https://programmablesearchengine.google.com)
6. Crea un nuevo motor de b√∫squeda
7. Obt√©n tu Search Engine ID (CX)

---

## üöÄ Instalaci√≥n y Ejecuci√≥n

### Prerrequisitos

- **Docker** y **Docker Compose** instalados
- **Git** para clonar el repositorio
- APIs configuradas (OpenAI + Google)

### Ejecuci√≥n con Docker (Recomendado)

```bash
# 1. Clonar el repositorio
git clone <repository-url>
cd project

# 2. Configurar variables de entorno
cp .env.example .env
# Edita .env con tus API keys

# 3. Construir y ejecutar todos los servicios
docker-compose up --build

# 4. Acceder a la aplicaci√≥n
# Frontend: http://localhost:8501
# API Orchestrator: http://localhost:8000
# Redis: http://localhost:6379
```

### Ejecuci√≥n Local (Desarrollo)

#### Frontend
```bash
cd src/frontend
pip install -r requirements.txt
streamlit run main.py --server.port=8501
```

#### Orchestrator
```bash
cd src/orchestrator
pip install -r requirements.txt
python -m spacy download en_core_web_sm
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

#### Scraper
```bash
cd src/scraper
pip install -r requirements.txt
playwright install
playwright install-deps
uvicorn main:app --host 0.0.0.0 --port 8002
```

#### Redis
```bash
# Con Docker
docker run -d -p 6379:6379 redis/redis-stack

# O instalaci√≥n local de Redis Stack
```

### Verificaci√≥n de la Instalaci√≥n

```bash
# Verificar que todos los servicios est√©n ejecut√°ndose
curl http://localhost:8000/docs          # API docs del Orchestrator
curl http://localhost:8501              # Frontend Streamlit
curl -X POST http://localhost:8002/scrape?url=https://example.com  # Scraper
```

---

## üìñ API Documentation

### Orchestrator API

La API principal del sistema expone un endpoint de streaming para consultas:

#### **GET** `/streamingSearch`

Procesa una consulta y retorna respuesta en streaming usando Server-Sent Events.

**Par√°metros:**
- `query` (string, required): La consulta del usuario

**Response:** Stream de eventos SSE con los siguientes tipos:

```typescript
// Evento: Resultados de b√∫squeda
{
  "event": "search",
  "data": {
    "items": [
      {
        "link": "https://example.com",
        "title": "Page Title",
        "snippet": "Description...",
        "displayLink": "example.com"
      }
    ]
  }
}

// Evento: Contexto recuperado
{
  "event": "context", 
  "data": "Texto contextual combinado de todas las fuentes..."
}

// Evento: Prompt final
{
  "event": "prompt",
  "data": "Use the following pieces of context to answer..."
}

// Evento: Token de respuesta (streaming)
{
  "event": "token",
  "data": "palabra"
}
```

#### **Ejemplo de uso con curl:**

```bash
curl -N -H "Accept: text/event-stream" \
  "http://localhost:8000/streamingSearch?query=What%20is%20machine%20learning"
```

### Scraper API

#### **POST** `/scrape`

Extrae contenido HTML de una URL espec√≠fica.

**Par√°metros:**
- `url` (string, required): URL a procesar

**Response:**
```json
{
  "html": "<html>...</html>"
}
```

**C√≥digos de Error:**
- `408`: Timeout (p√°gina muy lenta)
- `500`: Error de scraping

### OpenAPI Specification

````yaml
openapi: 3.0.0
info:
  title: InternetWhisper API
  description: RAG-powered chatbot with real-time web search capabilities
  version: 1.0.0
  license:
    name: MIT
    url: https://opensource.org/licenses/MIT

servers:
  - url: http://localhost:8000
    description: Local development server

paths:
  /streamingSearch:
    get:
      summary: Stream search and generation response
      description: |
        Processes a user query through the complete RAG pipeline:
        1. Searches vector cache for similar content
        2. If insufficient, performs web search
        3. Scrapes and processes web content  
        4. Generates embeddings and stores in cache
        5. Builds context and streams LLM response
      parameters:
        - name: query
          in: query
          required: true
          schema:
            type: string
            example: "What are the latest trends in artificial intelligence?"
          description: User's search query
      responses:
        '200':
          description: Successful streaming response
          content:
            text/event-stream:
              schema:
                type: string
                description: Server-Sent Events stream
              examples:
                search_event:
                  summary: Search results event
                  value: |
                    event: search
                    data: {"items":[{"link":"https://example.com","title":"AI Trends"}]}
                
                context_event:
                  summary: Retrieved context
                  value: |
                    event: context  
                    data: Artificial intelligence trends include...
                
                token_event:
                  summary: Generated token
                  value: |
                    event: token
                    data: Based

components:
  schemas:
    SearchResult:
      type: object
      properties:
        items:
          type: array
          items:
            $ref: '#/components/schemas/SearchDoc'
    
    SearchDoc:
      type: object
      properties:
        link:
          type: string
          format: uri
        title:
          type: string
        snippet:
          type: string
        displayLink:
          type: string

---

## üí¨ Ejemplos de Uso

### Interacci√≥n B√°sica

**Usuario:** "¬øCu√°les son las √∫ltimas tendencias en inteligencia artificial?"

**Sistema:** 
1. üîç *Buscando informaci√≥n relevante...*
2. üìÑ *Encontradas 8 fuentes sobre IA*
3. ü§ñ *Generando respuesta...*

**Respuesta:** "Seg√∫n las fuentes m√°s recientes, las principales tendencias en IA incluyen:

1. **IA Generativa**: Modelos como GPT-4 y Claude est√°n revolucionando la creaci√≥n de contenido...
2. **Multimodalidad**: Sistemas que pueden procesar texto, im√°genes y audio simult√°neamente...
3. **IA Responsable**: Mayor enfoque en la √©tica y transparencia de los algoritmos..."

### Consulta T√©cnica Espec√≠fica

**Usuario:** "¬øC√≥mo funciona el algoritmo de atenci√≥n en los transformers?"

**Sistema:**
- Busca en papers acad√©micos recientes
- Extrae informaci√≥n de blogs t√©cnicos especializados  
- Sintetiza explicaciones de m√∫ltiples fuentes
- Proporciona una explicaci√≥n comprensible con ejemplos

### Investigaci√≥n de Mercado

**Usuario:** "¬øQu√© empresas est√°n liderando el desarrollo de veh√≠culos aut√≥nomos en 2024?"

**Sistema:**
- Consulta noticias financieras actualizadas
- Revisa reports de la industria
- Analiza comunicados de prensa recientes
- Presenta un an√°lisis comprehensivo con datos actuales

---

## üìÅ Estructura del Proyecto

```
project/
‚îú‚îÄ‚îÄ üìÑ README.md                          # Este archivo
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml                 # Orquestaci√≥n de servicios
‚îú‚îÄ‚îÄ üìÑ .env.example                       # Template de variables de entorno
‚îú‚îÄ‚îÄ üìÑ pyproject.toml                     # Configuraci√≥n del proyecto
‚îú‚îÄ‚îÄ üìÑ LICENSE                            # Licencia MIT
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ frontend/                       # Servicio de interfaz de usuario
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py                     # Aplicaci√≥n Streamlit principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Dependencias del frontend
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ Dockerfile                  # Imagen Docker del frontend
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ orchestrator/                   # Servicio n√∫cleo del sistema RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py                     # API FastAPI principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Dependencias del orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile                  # Imagen Docker del orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ logging.conf                # Configuraci√≥n de logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/                     # Modelos de datos Pydantic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ document.py             # Modelo para documentos vectoriales
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ search.py               # Modelos para resultados de b√∫squeda
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ prompt/                     # Templates de prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ prompt.py               # Template RAG principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ retrieval/                  # Componentes del sistema RAG
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ retriever.py            # Orquestador principal RAG
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ search.py               # Integraci√≥n Google Search API
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scraper.py              # Scrapers local y remoto
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ embeddings.py           # Clientes de embeddings
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ splitter.py             # Estrategias de segmentaci√≥n
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ cache.py                # Cache vectorial Redis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ util/                       # Utilidades del sistema
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ logger.py               # Configuraci√≥n de logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ mocks/                      # Datos de prueba
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ test_dict.py            # Resultados mock para testing
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ scraper/                        # Servicio de web scraping
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ main.py                     # API FastAPI para scraping
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Dependencias del scraper
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ Dockerfile                  # Imagen Docker del scraper
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ nginx.conf                  # Configuraci√≥n proxy (opcional)
‚îú‚îÄ‚îÄ üìÅ tests/                             # Directorio de tests (pendiente)
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ __init__.py
‚îî‚îÄ‚îÄ üìÅ redis_data/                        # Persistencia de Redis
    ‚îî‚îÄ‚îÄ üìÑ dump.rdb                       # Snapshot de la base de datos
```

### Descripci√≥n de Componentes Clave

#### **Retrieval System** ([`src/orchestrator/retrieval/`](project/src/orchestrator/retrieval/))

- **[`retriever.py`](project/src/orchestrator/retrieval/retriever.py)**: Coordinador principal que implementa el flujo RAG completo
- **[`cache.py`](project/src/orchestrator/retrieval/cache.py)**: Implementa cache vectorial con Redis y b√∫squeda por similitud
- **[`search.py`](project/src/orchestrator/retrieval/search.py)**: Integraci√≥n con Google Custom Search API
- **[`scraper.py`](project/src/orchestrator/retrieval/scraper.py)**: Abstractiones para scraping local y remoto
- **[`embeddings.py`](project/src/orchestrator/retrieval/embeddings.py)**: Clientes para generar embeddings (OpenAI, local)
- **[`splitter.py`](project/src/orchestrator/retrieval/splitter.py)**: Estrategias de segmentaci√≥n de texto

#### **Models** ([`src/orchestrator/models/`](project/src/orchestrator/models/))

- **[`document.py`](project/src/orchestrator/models/document.py)**: Representa documentos con vectores y metadatos
- **[`search.py`](project/src/orchestrator/models/search.py)**: Estructura para resultados de b√∫squeda web

---

## üß™ Testing y Desarrollo

### Configuraci√≥n de Testing (Recomendaci√≥n)

El proyecto actualmente no incluye tests, pero ser√≠a altamente recomendable implementar:

```python
# tests/test_retriever.py - Ejemplo de estructura recomendada
import pytest
from orchestrator.retrieval.retriever import Retriever

@pytest.mark.asyncio
async def test_retriever_cache_hit():
    """Test que el retriever use cache cuando hay similarity alta"""
    # Mock setup
    # Test implementation
    pass

@pytest.mark.asyncio  
async def test_retriever_web_search():
    """Test flujo completo de b√∫squeda web"""
    # Mock Google API, scraper, embeddings
    # Test implementation
    pass
```

### Configuraci√≥n de Desarrollo

```bash
# Instalar dependencias de desarrollo
pip install pytest pytest-asyncio httpx

# Ejecutar tests
pytest tests/ -v

# Linting y formato
pip install black isort flake8
black src/
isort src/
flake8 src/
```

---

## üîß Configuraci√≥n Avanzada

### Ajuste de Par√°metros RAG

Puedes modificar los par√°metros del sistema en [`src/orchestrator/main.py`](project/src/orchestrator/main.py):

```python
# Configuraci√≥n del cache threshold
cache_treshold = 0.85  # Umbral de similitud para usar cache

# Configuraci√≥n del retriever
k = 10  # N√∫mero de documentos a recuperar

# Configuraci√≥n del splitter
chunk_size = 400      # Tama√±o de chunks en caracteres
chunk_overlap = 50    # Overlap entre chunks
```

### Variables de Entorno Opcionales

```bash
# Timeouts y configuraci√≥n HTTP
SCRAPER_TIMEOUT=5000
HTTP_MAX_RETRIES=3
REDIS_TTL=3600

# Configuraci√≥n de modelos
OPENAI_MODEL="gpt-3.5-turbo"
EMBEDDING_MODEL="text-embedding-ada-002"
EMBEDDING_DIMENSIONS=1536

# Configuraci√≥n de logging
LOG_LEVEL="INFO"
LOG_FORMAT="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

---

## üö¶ Monitoreo y Observabilidad

### Logs del Sistema

Cada servicio genera logs estructurados:

```bash
# Ver logs en tiempo real
docker-compose logs -f orchestrator
docker-compose logs -f frontend  
docker-compose logs -f scraper

# Logs espec√≠ficos
docker-compose logs orchestrator | grep "RETRIEVAL SCORE"
docker-compose logs orchestrator | grep "CACHE SCORE"
```

### M√©tricas Importantes

El sistema reporta m√©tricas clave como:

- **CACHE SCORE**: Puntuaci√≥n de similitud del cache (0-1)
- **RETRIEVAL SCORE**: Calidad promedio de documentos recuperados
- **SCRAPE TIME**: Tiempo de extracci√≥n de contenido web
- **EMBEDDING TIME**: Tiempo de generaci√≥n de embeddings
- **SCRAPED PAGES**: N√∫mero de p√°ginas procesadas exitosamente

---

## ü§ù Contribuci√≥n

### C√≥mo Contribuir

1. **Fork** el repositorio
2. **Crea** una rama para tu feature (`git checkout -b feature/amazing-feature`)
3. **Commit** tus cambios (`git commit -m 'Add amazing feature'`)
4. **Push** a la rama (`git push origin feature/amazing-feature`)
5. **Abre** un Pull Request

### √Åreas de Mejora

- **Testing**: Implementar suite completa de tests unitarios e integraci√≥n
- **Monitoring**: A√±adir m√©tricas con Prometheus/Grafana
- **Caching**: Optimizar estrategias de invalidaci√≥n de cache
- **Error Handling**: Mejorar manejo de errores y fallbacks
- **Performance**: Optimizar paralelizaci√≥n de scraping
- **Security**: Implementar rate limiting y validaci√≥n de URLs

### Est√°ndares de C√≥digo

- **Python**: Seguir PEP 8, usar `black` para formato
- **Type Hints**: Usar anotaciones de tipo en todo el c√≥digo
- **Docstrings**: Documentar funciones y clases p√∫blicas
- **Testing**: Mantener cobertura > 80%
- **Git**: Commits at√≥micos con mensajes descriptivos

---

## üìÑ Licencia

Este proyecto est√° licenciado bajo la **MIT License** - ver el archivo [LICENSE](LICENSE) para m√°s detalles.

---

## üìû Soporte y Contacto

- **Issues**: [GitHub Issues](../../issues)
- **Documentaci√≥n**: Este README y docstrings en el c√≥digo
- **API Docs**: http://localhost:8000/docs (cuando est√© ejecut√°ndose)

---

## üôè Agradecimientos

Este proyecto utiliza las siguientes tecnolog√≠as y servicios:

- **OpenAI** por los modelos GPT-3.5-turbo y text-embedding-ada-002
- **Google** por la Custom Search API
- **Redis** por la infraestructura de cache vectorial
- **Streamlit** por el framework de UI
- **FastAPI** por el framework web async
- **LangChain** por las utilidades de procesamiento de texto
- **Playwright** por las capacidades de web scraping

---

*Documentaci√≥n generada con la asistencia de **GitHub Copilot** ü§ñ*

---

**‚≠ê Si este proyecto te resulta √∫til, no olvides darle una estrella en GitHub!**